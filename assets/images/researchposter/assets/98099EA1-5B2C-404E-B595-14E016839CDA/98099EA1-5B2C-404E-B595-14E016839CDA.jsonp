local_slide( {"name":"98099EA1-5B2C-404E-B595-14E016839CDA","json":{"assets":{"ACF482A3395CC98E9138DF491E18B2F7":{"type":"texture","index":0,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/98099EA1-5B2C-404E-B595-14E016839CDA.pdf"},"width":3456,"height":2592},"A49CA3FC88CFEF0B4C3FBA93DA9C6B63":{"type":"texture","index":1,"assetRequest":{"type":"slide","state":"contents","slide":"none"},"url":{"native":"assets\/98099EA1-5B2C-404E-B595-14E016839CDA.pdf"},"width":3456,"height":2592}},"events":[{"effects":[{"beginTime":0,"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00014586630194489994,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"A49CA3FC88CFEF0B4C3FBA93DA9C6B63"},{"animations":[{"timeOffset":0,"from":{"scalar":false},"beginTime":0,"repeatCount":0,"fillMode":"both","property":"hidden","autoreverses":false,"duration":0.01,"to":{"scalar":true}}],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"ACF482A3395CC98E9138DF491E18B2F7"}]}]},"effects":[],"duration":0.01,"type":"transition","attributes":{"direction":0},"name":"none","objectID":"0"}],"automaticPlay":false,"hyperlinks":[],"accessibility":[{"text":"Figure 2: Inputs and outputs of the Automatic Rendering process. ","targetRectangle":{"y":1020.176052570343,"x":1202.7031064033508,"width":524.3115234375,"height":20.4111328125}},{"text":"Google Shape;12;p1","targetRectangle":{"y":2351.57861328125,"x":2134.736572265625,"width":151.22244262695312,"height":152.01283264160156}},{"text":"Figure 2: Alignment path for the first four measures of Twinkle Twinkle Little Star.","targetRectangle":{"y":2266.0335354804993,"x":184.98423433303833,"width":639.6416015625,"height":20.4111328125}},{"text":"This research was funded by the National Science Foundation.","targetRectangle":{"y":2414.2163450717926,"x":1295.376220703125,"width":833.943359375,"height":31.083984375}},{"text":"Introduction","targetRectangle":{"y":409.66928696632385,"x":150.59453415870667,"width":376.2421875,"height":79.64453125}},{"text":"Using music feature extraction and audio-to-score alignment tools, we have developed a software which is capable of triggering audiovisual events based on a musician’s position in a piece. Information is provided to the program in MusicXML format and matched to real-time audio performance, which triggers augmented events.  ","targetRectangle":{"y":525.66928696632385,"x":150.59453415870667,"width":879.306640625,"height":299.7568359375}},{"text":"Motivation","targetRectangle":{"y":914.66928696632385,"x":150.59453415870667,"width":328.1484375,"height":79.64453125}},{"text":"Video Jockey (VJ) software is used by musicians to provide their audiences with an immersive audiovisual experience. Many of these software allow musicians to trigger events based on the musical features of their performance, using OSC or MIDI messages. Musicians who play traditional analogue instruments are interested in utilizing VJ  tools during their performances, but without a way to connect their instruments to these software, performances must be facilitated with the help of a video operator. Operators are limited to triggering every event manually, following along with the score, which can be tedious and limits the range of events that can be triggered. ","targetRectangle":{"y":1030.6692869663239,"x":150.59453415870667,"width":873.666015625,"height":600.7568359375}},{"text":"Google Shape;33;p3","targetRectangle":{"y":468.42583465576172,"x":1195.504264831543,"width":2056.596923828125,"height":548.32763671875}},{"text":" Method","targetRectangle":{"y":1127.6852171421051,"x":1173.7844603061676,"width":260.15625,"height":79.64453125}},{"text":"Developed procedure for extracting chroma features from MusicXML (format often used to store and share musical scores, accessible to musicians)","targetRectangle":{"y":1243.6852171421051,"x":1209.7844603061676,"width":924.451171875,"height":124.822265625}},{"text":"Extracted chroma features from audio using the chroma_cqt function contained in the audio analysis library librosa, discussed in McFee 2015. ","targetRectangle":{"y":1411.6852171421051,"x":1209.7844603061676,"width":966.48046875,"height":124.822265625}},{"text":"Frame size for audio and score analysis is limited to 1ms. ","targetRectangle":{"y":1579.6852171421051,"x":1209.7844603061676,"width":924.36328125,"height":40.822265625}},{"text":"Implemented the Online-Time Warping algorithm as presented in Dixon, 2005. ","targetRectangle":{"y":1663.6852171421051,"x":1209.7844603061676,"width":831.673828125,"height":82.822265625}},{"text":"UDP client sends OSC messages based on most recent score index matched. Client also listens for messages from server (in our case, a QLab workstation) and may change position in analysis based on feedback. (Users may still trigger cues manually, this must be communicated to alignment function if there is a delay)","targetRectangle":{"y":1789.6852171421051,"x":1209.7844603061676,"width":950.396484375,"height":250.822265625}},{"text":"Conclusion ","targetRectangle":{"y":1127.6850950717926,"x":2297.1072142124176,"width":316.01390625000022,"height":68.021406249999927}},{"text":"Our software successfully aligns a sequence to itself, as well as a score-based sequence to an audio-based sequence. The software is capable of sending OSC messages based on the position in analysis, but does not yet react to OSC feedback. Further testing is needed to compare the efficacy of score-to-audio alignment compared to audio-to-audio alignment methods. GUI software for end-user still needs to be implemented, as well as the ability to change the position of the analysis dynamically to arbitrary positions in the score. One potential test for accuracy could be the ratio of user interventions to successful automatic matches. ","targetRectangle":{"y":1225.6850950717926,"x":2297.1072142124176,"width":1002.9698437500001,"height":305.01070312499996}},{"text":"Sources","targetRectangle":{"y":1566.6850950717926,"x":2297.1072142124176,"width":147.91218750000007,"height":45.680937500000027}},{"text":"B. McFee. “librosa: Audio and Music Signal Analysis in Python” Proc. Of The 14th Python In Science Conf., 2015, pp. 18-24.","targetRectangle":{"y":1613.6850950717926,"x":2297.1072142124176,"width":957.7082812499998,"height":45.340468750000127}},{"text":"S. Dixon. “Live-Tracking of Musical Performances Using On-Line Time Warping” Proc. of the 8th Int. Conference on Digital Audio Effects (DAFx’05), Madrid, Spain, September 2005, pp.20-22","targetRectangle":{"y":1682.6850950717926,"x":2297.1072142124176,"width":1003.5506249999999,"height":45.340468750000127}},{"text":"Acknowledgements","targetRectangle":{"y":1751.6850950717926,"x":2297.1072142124176,"width":349.63031249999995,"height":45.680937500000027}},{"text":"This work was completed in the Audio Information Research lab at the University of Rochester, dept. of Electrical and Computer Engineering during the 2019 NSF REU in Computational Methods for Music, Media, and Minds. I would like to thank Christodoulos Benetatos and Bochen Li of the AIR lab for their wisdom during the debugging process and for their help implementing the OTW algorithm. I would also like to thank Christopher Winders of TableTopOpera for his support developing and testing the QLab communication feature of the software. ","targetRectangle":{"y":1798.6850950717926,"x":2297.1072142124176,"width":992.51578125000015,"height":137.34046875000013}},{"text":"Google Shape;10;p1","targetRectangle":{"y":2351.57861328125,"x":2889.93115234375,"width":374.33132934570312,"height":152.07212829589844}},{"text":"Google Shape;36;p3","targetRectangle":{"y":1694.4961624145508,"x":160.93897247314453,"width":776.375,"height":554.8447265625}},{"text":"Nick Creel, Marlboro College","targetRectangle":{"y":263.93307161331177,"x":1396.1438827514648,"width":613.546875,"height":53.4296875}},{"text":"Advisors: Zhiyao Duan, University of Rochester (ECE) & Matthew Brown, University of Rochester (Music)","targetRectangle":{"y":318.93307161331177,"x":586.56575775146484,"width":2232.703125,"height":53.4296875}},{"text":"Google Shape;11;p1","targetRectangle":{"y":2379.40478515625,"x":200.78291320800781,"width":482.48306274414062,"height":99.443069458007812}},{"text":"Automatic Rendering of Augmented Events in Immersive Concerts","targetRectangle":{"y":109.0984251499176,"x":260.6416015625,"width":2934.716796875,"height":111.728515625}}],"baseLayer":{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"objectID":"0","layers":[{"animations":[],"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,-0.00014586630194489994,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":251658240,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"layers":[{"animations":[],"layers":[],"texturedRectangle":{"isBackgroundTexture":false,"singleTextureOpacity":1,"textureType":0,"textBaseline":0,"textXHeight":0,"isVerticalText":false},"initialState":{"affineTransform":[1,0,0,1,0,0],"masksToBounds":false,"rotation":0,"scale":1,"position":{"pointX":1728,"pointY":1296},"width":3456,"sublayerTransform":[1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1],"contentsRect":{"y":0,"x":0,"width":1,"height":1},"opacity":1,"edgeAntialiasingMask":0,"height":2592,"hidden":false,"anchorPoint":{"pointX":0.5,"pointY":0.5}},"texture":"ACF482A3395CC98E9138DF491E18B2F7"}]}]}}]}} )